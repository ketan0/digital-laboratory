<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-12-16 Sat 01:32 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>causal inference</title>
<meta name="author" content="Ketan Agrawal" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="syntax.css" />
<link rel="stylesheet" type="text/css" href="styles.css" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
<link rel="manifest" href="/site.webmanifest" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<header>
    <script src="setup-initial-theme.js"></script>
    <div style="display: flex; flex-direction: row; justify-content: space-between; align-items: center;">
        <a style="color: inherit; text-decoration: none" href="/">
            ketan agrawal
        </a>
        <div>
            <input type="checkbox" id="theme-switcher">
            <label id="theme-switcher-label" for="theme-switcher"></label>
        </div>
    </div>
</header>
</div>
<div id="content" class="content">
<h1 class="title">causal inference
<br />
<span class="subtitle">Last modified on May 09, 2022</span>
</h1>

<div id="outline-container-ID-d4b17339-7852-4eb6-a399-24e47b354a6c" class="outline-2">
<h2 id="ID-d4b17339-7852-4eb6-a399-24e47b354a6c">observation</h2>
<div class="outline-text-2" id="text-org45a66e1">
</div>

<div id="outline-container-orgce9df65" class="outline-3 references">
<h3 id="orgce9df65">Links to &ldquo;observation&rdquo;</h3>
<div class="outline-text-3" id="text-orgce9df65">
</div>
<div id="outline-container-org7351a40" class="outline-4">
<h4 id="org7351a40"><a href="machine_learning.html#ID-5b02540a-15ac-4123-86f8-e6ca5420ce27">machine learning</a></h4>
<div class="outline-text-4" id="text-org7351a40">
<p>
Supervised learning is an example of <a href="#ID-d4b17339-7852-4eb6-a399-24e47b354a6c">observational</a> inference &#x2013; we&rsquo;re just looking for associations between variables \(X\) and \(Y\). Aka, we&rsquo;re just learning \(P(Y|X)\).<br />
</p>

<p>
I feel like this thread captures a really interesting divide / contrast of philosophies in machine learning research:<br />
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Researchers in speech recognition, computer vision, and natural language processing in the 2000s were obsessed with accurate representations of uncertainty. <br>1/N</p>&mdash; Yann LeCun (@ylecun) <a href="https://twitter.com/ylecun/status/1525560489216028677?ref_src=twsrc%5Etfw">May 14, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>
My goal now is to deeply understand the issues at hand in this thread. I found his mention of factor graphs in the shift to reasoning and planning AI was thought-provoking. I feel that causality and factor graphs and Bayesian and all that are very important. I just don&rsquo;t know quite enough to put the pieces together yet.<br />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-ID-d68c5093-d6d6-43b8-a48d-629ade9293b6" class="outline-2">
<h2 id="ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</h2>
<div class="outline-text-2" id="text-org03d00f7">
</div>

<div id="outline-container-org3f83f41" class="outline-3 references">
<h3 id="org3f83f41">Links to &ldquo;intervention&rdquo;</h3>
<div class="outline-text-3" id="text-org3f83f41">
</div>
<div id="outline-container-org969cc26" class="outline-4">
<h4 id="org969cc26"><a href="ablation_studies.html#ID-766f764c-818b-41ff-a7ed-02642696a830">ablation studies</a></h4>
<div class="outline-text-4" id="text-org969cc26">
<p>
Ablation studies are effectively using <a href="#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">interventions</a> (removing parts of your system) to reveal the underlying causal structure of your system. Francois Chollet (creator of Keras) writes about this being useful in a <a href="machine_learning.html#ID-5b02540a-15ac-4123-86f8-e6ca5420ce27">machine learning</a> context:<br />
</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Ablation studies are crucial for deep learning research -- can&#39;t stress this enough.<br><br>Understanding causality in your system is the most straightforward way to generate reliable knowledge (the goal of any research). And ablation is a very low-effort way to look into causality.</p>&mdash; Fran√ßois Chollet (@fchollet) <a href="https://twitter.com/fchollet/status/1012721582148550662?ref_src=twsrc%5Etfw">June 29, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</div>

<div id="outline-container-org5a83538" class="outline-4">
<h4 id="org5a83538"><a href="nancy_kanwisher.html#ID-fb920b99-a2b5-4a20-bf7c-875727e6ae58">Nancy Kanwisher</a> <span class="backlinks-outline-path">(<i>A roadmap for research &gt; Causal role?</i>)</span></h4>
<div class="outline-text-4" id="text-org5a83538">
<p>
fMRI is nice&#x2026;but what&rsquo;s the <i>causal</i> role of these regions? We don&rsquo;t just want correlations of brain activations &amp; activities.<br />
</p>

<p>
We need experiments where we &ldquo;poke&rdquo; part of the system &#x2013; <a href="#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a>. Schalk et al.<br />
</p>

<p>
If he&rsquo;s looking at a face, the face changes. If he&rsquo;s looking at something else, it adds a face to that object.<br />
</p>

<p>
&ldquo;Poking the face area&rdquo; results in weird, weird face stuff happening to brain patient<br />
</p>

<p>
Stimulated color regions &#x2013; he saw a rainbow (wtfff)<br />
</p>
</div>
</div>

<div id="outline-container-org62f2d02" class="outline-4">
<h4 id="org62f2d02"><a href="towards_causal_representation_learning.html#ID-12dfdb1e-d4ed-476b-be04-98cae7a3deaf">Towards Causal Representation Learning</a> <span class="backlinks-outline-path">(<i>Independent mechanisms</i>)</span></h4>
<div class="outline-text-4" id="text-org62f2d02">
<p>
Hypothesis: We can explain the world by the composition of informationally independent pieces/modules/mechanisms. (Note: not statistically independent, but independent s.t. any causal <a href="#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a> would affect just one such mechanism.)<br />
</p>
</div>
</div>

<div id="outline-container-orgd7fe2cd" class="outline-4">
<h4 id="orgd7fe2cd"><a href="towards_causal_representation_learning.html#ID-12dfdb1e-d4ed-476b-be04-98cae7a3deaf">Towards Causal Representation Learning</a> <span class="backlinks-outline-path">(<i>Causal induction from interventional data</i>)</span></h4>
<div class="outline-text-4" id="text-orgd7fe2cd">
<p>
How to handle unknown <a href="#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a>? <i>infer</i> it.<br />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-ID-1f3f1a31-ff89-4c05-8c82-64888887f45e" class="outline-2">
<h2 id="ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactual</h2>
<div class="outline-text-2" id="text-orgbf4c680">
</div>


<div id="outline-container-org43d26a0" class="outline-3 references">
<h3 id="org43d26a0">Links to &ldquo;counterfactual&rdquo;</h3>
<div class="outline-text-3" id="text-org43d26a0">
</div>
<div id="outline-container-org91679d6" class="outline-4">
<h4 id="org91679d6"><a href="counterfactual_generative_networks.html#ID-22706d1f-6b5c-4c77-acc2-d8c222b395d5">Counterfactual Generative Networks</a></h4>
<div class="outline-text-4" id="text-org91679d6">
<p>
<a href="neural_networks_like_to_cheat.html#ID-412cda14-f385-463d-9a7e-cd9ffe87c0a2">Neural networks like to &ldquo;cheat&rdquo;</a> by using simple correlations that fail to generalize. E.g., image classifiers can learn spurious correlations with texture in the background, rather than the actual object&rsquo;s shape; a classifier might learn that &ldquo;green grass background&rdquo; =&gt; &ldquo;cow classification.&rdquo;<br />
</p>

<p>
This work <a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">decomposes</a> the image generation process into three independent causal mechanisms &#x2013; shape, texture, and background. Thus, one can generate &ldquo;<a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactual</a> images&rdquo; to improve OOD robustness, e.g. by placing a cow on a swimming pool background. Related: <a href="private/20200928215821-psych_204.html#ID-8e87ac0e-1002-474e-b4e7-778d908270a6">generative models</a> <a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactuals</a><br />
</p>
</div>
</div>

<div id="outline-container-org6b89e1f" class="outline-4">
<h4 id="org6b89e1f"><a href="counterfactual_generative_networks.html#ID-22706d1f-6b5c-4c77-acc2-d8c222b395d5">Counterfactual Generative Networks</a></h4>
<div class="outline-text-4" id="text-org6b89e1f">
<p>
<a href="neural_networks_like_to_cheat.html#ID-412cda14-f385-463d-9a7e-cd9ffe87c0a2">Neural networks like to &ldquo;cheat&rdquo;</a> by using simple correlations that fail to generalize. E.g., image classifiers can learn spurious correlations with texture in the background, rather than the actual object&rsquo;s shape; a classifier might learn that &ldquo;green grass background&rdquo; =&gt; &ldquo;cow classification.&rdquo;<br />
</p>

<p>
This work <a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">decomposes</a> the image generation process into three independent causal mechanisms &#x2013; shape, texture, and background. Thus, one can generate &ldquo;<a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactual</a> images&rdquo; to improve OOD robustness, e.g. by placing a cow on a swimming pool background. Related: <a href="private/20200928215821-psych_204.html#ID-8e87ac0e-1002-474e-b4e7-778d908270a6">generative models</a> <a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactuals</a><br />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer style="font-size: 0.75rem;">
    <p>Made with <span class="heart">‚ô•</span> using
        <a href="https://orgmode.org/">org-mode</a>.
        Source code is available
        <a href="https://github.com/ketan0/digital-laboratory">here</a>.
    </p>
</footer>
<script src="popper.min.js"></script>
<script src="tippy-bundle.umd.min.js"></script>
<script src="tooltips.js"></script>
<script src="setup-theme-switcher.js"></script>
<script src="insert-intext-citation.js"></script>
</div>
</body>
</html>