<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-03-02 Wed 20:50 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>causal inference</title>
<meta name="author" content="Ketan Agrawal" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="syntax.css" />
<link rel="stylesheet" type="text/css" href="styles.css" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
<link rel="manifest" href="/site.webmanifest" />
</head>
<body>
<div id="preamble" class="status">
<a style="color: inherit; text-decoration: none" href="/"><h2>Ketan's Nebula</h2></a>
</div>
<div id="content" class="content">
<h1 class="title">causal inference</h1>

<div id="outline-container-ID-d4b17339-7852-4eb6-a399-24e47b354a6c" class="outline-2">
<h2 id="ID-d4b17339-7852-4eb6-a399-24e47b354a6c">observation</h2>
<div class="outline-text-2" id="text-orgc83454c">
</div>
</div>
<div id="outline-container-ID-d68c5093-d6d6-43b8-a48d-629ade9293b6" class="outline-2">
<h2 id="ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</h2>
<div class="outline-text-2" id="text-orgdffc96a">
</div>

<div id="outline-container-orgb8cbbb6" class="outline-3 references">
<h3 id="orgb8cbbb6">Links to this node</h3>
<div class="outline-text-3" id="text-orgb8cbbb6">
</div>
<div id="outline-container-orgcde5aac" class="outline-4">
<h4 id="orgcde5aac"><a href="towards_causal_representation_learning.html#ID-ea661fe1-d0f4-4bf8-9678-0cbbe9f73fc5">Independent mechanisms</a></h4>
<div class="outline-text-4" id="text-orgcde5aac">
<p>
Hypothesis: We can explain the world by the composition of informationally independent pieces/modules/mechanisms. (Note: not statistically independent, but independent s.t. any causal <a href="#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a> would affect just one such mechanism.)<br />
</p>
</div>
</div>

<div id="outline-container-org24395b5" class="outline-4">
<h4 id="org24395b5"><a href="towards_causal_representation_learning.html#ID-12dfdb1e-d4ed-476b-be04-98cae7a3deaf">Towards Causal Representation Learning</a></h4>
<div class="outline-text-4" id="text-org24395b5">
<p>
How to handle unknown <a href="#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a>? <i>infer</i> it.<br />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-ID-1f3f1a31-ff89-4c05-8c82-64888887f45e" class="outline-2">
<h2 id="ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactual</h2>
<div class="outline-text-2" id="text-org6e8506e">
</div>


<div id="outline-container-org0894777" class="outline-3 references">
<h3 id="org0894777">Links to this node</h3>
<div class="outline-text-3" id="text-org0894777">
</div>
<div id="outline-container-org56b4072" class="outline-4">
<h4 id="org56b4072"><a href="counterfactual_generative_networks.html#ID-22706d1f-6b5c-4c77-acc2-d8c222b395d5">Counterfactual Generative Networks</a></h4>
<div class="outline-text-4" id="text-org56b4072">
<p>
(, )<br />
</p>

<p>
<a href="neural_networks_like_to_cheat.html#ID-412cda14-f385-463d-9a7e-cd9ffe87c0a2">Neural networks like to "cheat"</a> by using simple correlations that fail to generalize. E.g., image classifiers can learn spurious correlations with texture in the background, rather than the actual object's shape; a classifier might learn that "green grass background" =&gt; "cow classification."<br />
</p>

<p>
This work <a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">decomposes</a> the image generation process into three independent causal mechanisms &#x2013; shape, texture, and background. Thus, one can generate "<a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactual</a> images" to improve OOD robustness, e.g. by placing a cow on a swimming pool background. Related: <a href="private/20200928215821-psych_204.html#ID-8e87ac0e-1002-474e-b4e7-778d908270a6">generative models</a> <a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactuals</a><br />
</p>
</div>
</div>

<div id="outline-container-org8c41cf3" class="outline-4">
<h4 id="org8c41cf3"><a href="counterfactual_generative_networks.html#ID-22706d1f-6b5c-4c77-acc2-d8c222b395d5">Counterfactual Generative Networks</a></h4>
<div class="outline-text-4" id="text-org8c41cf3">
<p>
(, )<br />
</p>

<p>
<a href="neural_networks_like_to_cheat.html#ID-412cda14-f385-463d-9a7e-cd9ffe87c0a2">Neural networks like to "cheat"</a> by using simple correlations that fail to generalize. E.g., image classifiers can learn spurious correlations with texture in the background, rather than the actual object's shape; a classifier might learn that "green grass background" =&gt; "cow classification."<br />
</p>

<p>
This work <a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">decomposes</a> the image generation process into three independent causal mechanisms &#x2013; shape, texture, and background. Thus, one can generate "<a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactual</a> images" to improve OOD robustness, e.g. by placing a cow on a swimming pool background. Related: <a href="private/20200928215821-psych_204.html#ID-8e87ac0e-1002-474e-b4e7-778d908270a6">generative models</a> <a href="#ID-1f3f1a31-ff89-4c05-8c82-64888887f45e">counterfactuals</a><br />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p>Made with <span class="heart">â™¥</span> using
<a href="https://orgmode.org/">org-mode</a>.
Source code is available
<a href="https://github.com/ketan0/digital-laboratory">here</a>.</p>
<script src="popper.min.js"></script>
<script src="tippy-bundle.umd.min.js"></script>
<script src="tooltips.js"></script>
</div>
</body>
</html>
