<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-05-14 Sat 13:46 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Towards Causal Representation Learning</title>
<meta name="author" content="Ketan Agrawal" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="syntax.css" />
<link rel="stylesheet" type="text/css" href="styles.css" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
<link rel="manifest" href="/site.webmanifest" />
</head>
<body>
<div id="preamble" class="status">
<header>
    <script src="setup-initial-theme.js"></script>
    <div style="display: flex; flex-direction: row; justify-content: space-between; align-items: center;">
        <a style="color: inherit; text-decoration: none" href="/">
            <svg id="nebula-logo" x="0px" y="0px" width="50px"
                 viewBox="0 0 290.513 290.513" style="enable-background:new 0 0 290.513 290.513;" xml:space="preserve">
                <g>
                    <g>
                        <path d="M168.684,103.086c-58.679,0-93.713,17.515-93.713,46.857c0,18.344,27.214,28.114,46.857,28.114
                                 c46.327,0,46.857-22.472,46.857-23.428l-9.367-0.141c-0.009,0.141-1.354,14.198-37.49,14.198
                                 c-16.976,0-37.485-8.359-37.485-18.743c0-23.124,32.317-37.485,84.342-37.485c68.312,0,112.456,20.233,112.456,51.542
                                 c0,32.818-50.989,70.252-145.49,74.975l0.469,9.357c101.36-5.065,154.393-46.262,154.393-84.333
                                 C290.511,126.996,242.689,103.086,168.684,103.086z"/>
                        <path d="M215.541,154.628c0-18.344-27.214-28.114-46.857-28.114c-46.327,0-46.857,22.472-46.857,23.428
                                 l9.367,0.141c0.009-0.141,1.354-14.198,37.49-14.198c16.976,0,37.485,8.359,37.485,18.743c0,23.124-32.317,37.485-84.342,37.485
                                 c-68.312,0-112.456-20.233-112.456-51.542c0-32.818,50.989-70.252,145.49-74.975l-0.469-9.357C53.032,61.303,0,102.5,0,140.571
                                 c0,37.003,47.822,60.914,121.827,60.914C180.506,201.485,215.541,183.97,215.541,154.628z"/>
                        <path d="M51.542,107.771c0,7.75-6.307,14.057-14.057,14.057v9.371c7.75,0,14.057,6.307,14.057,14.057h9.371
                                 c0-7.75,6.307-14.057,14.057-14.057v-9.371c-7.75,0-14.057-6.307-14.057-14.057H51.542z M56.228,131.214
                                 c-1.335-1.776-2.919-3.364-4.7-4.7c1.781-1.335,3.364-2.924,4.7-4.7c1.335,1.776,2.919,3.364,4.7,4.7
                                 C59.147,127.849,57.563,129.438,56.228,131.214z"/>
                        <path d="M238.969,187.428h9.371c0-10.337,8.406-18.743,18.743-18.743v-9.371
                                 c-10.337,0-18.743-8.406-18.743-18.743h-9.371c0,10.337-8.406,18.743-18.743,18.743v9.371
                                 C230.563,168.685,238.969,177.091,238.969,187.428z M243.655,156.095c2.08,3.13,4.775,5.82,7.905,7.905
                                 c-3.13,2.08-5.82,4.775-7.905,7.905c-2.08-3.13-4.775-5.82-7.905-7.905C238.88,161.919,241.574,159.225,243.655,156.095z"/>
                        <path d="M103.085,206.17h-9.371c0,10.337-8.406,18.743-18.743,18.743v9.371
                                 c10.337,0,18.743,8.406,18.743,18.743h9.371c0-10.337,8.406-18.743,18.743-18.743v-9.371
                                 C111.491,224.913,103.085,216.507,103.085,206.17z M98.399,237.503c-2.08-3.13-4.775-5.82-7.905-7.905
                                 c3.13-2.08,5.82-4.775,7.905-7.905c2.08,3.13,4.775,5.82,7.905,7.905C103.174,231.679,100.479,234.373,98.399,237.503z"/>
                        <path d="M182.741,84.343h9.371c0-10.337,8.406-18.743,18.743-18.743v-9.371
                                 c-10.337,0-18.743-8.406-18.743-18.743h-9.371c0,10.337-8.406,18.743-18.743,18.743V65.6
                                 C174.335,65.6,182.741,74.006,182.741,84.343z M187.427,53.01c2.08,3.13,4.775,5.82,7.905,7.905
                                 c-3.13,2.08-5.82,4.775-7.905,7.905c-2.08-3.13-4.775-5.82-7.905-7.905C182.652,58.834,185.346,56.14,187.427,53.01z"/>
                        <rect x="98.399" y="135.885" width="9.371" height="9.371"/>
                        <rect x="107.77" y="149.942" width="9.371" height="9.371"/>
                        <rect x="168.684" y="168.685" width="9.371" height="9.371"/>
                        <rect x="182.741" y="145.257" width="9.371" height="9.371"/>
		                    <rect x="215.541" y="187.428" width="9.371" height="9.371"/>
		                    <rect x="154.627" y="215.542" width="9.371" height="9.371"/>
	                  </g>
                </g>
            </svg>
        </a>
        <div>
            <input type="checkbox" id="theme-switcher">
            <label id="theme-switcher-label" for="theme-switcher"></label>
        </div>
    </div>
</header>
</div>
<div id="content" class="content">
<h1 class="title">Towards Causal Representation Learning
<br />
<span class="subtitle">Last modified on May 09, 2022</span>
</h1>
<p>
Yoshua Bengio <a href="https://www.youtube.com/watch?v=rKZJ0TJWvTk">talk</a>. Also, the associated <a href="https://arxiv.org/abs/2102.11107">paper</a>.<br />
</p>

<p>
causal representation learning: the discovery of high-level causal variables from low-level observations.<br />
</p>

<p>
In practice, i.i.d. is a bad assumption. Things don't stay the same distribution as train. Current DL systems are brittle.<br />
</p>

<p>
But&#x2026;what assumption can we replace it with, then?<br />
</p>

<p>
how does the brain break knowledge apart into "pieces" that can be reused? =&gt; <a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">compositionality</a> (thinking decomposition into helper functions in programming.) Examples of compositionality include<br />
</p>

<div id="outline-container-org39cf01d" class="outline-2 no-display">
<h2 id="org39cf01d">Link</h2>
<div class="outline-text-2" id="text-org39cf01d">
<p>
<sup><a href="#citeproc_bib_item_1">[1]</a></sup><br />
</p>
</div>
</div>
<div id="outline-container-org842a55e" class="outline-2">
<h2 id="org842a55e">Systematic Generalization</h2>
<div class="outline-text-2" id="text-org842a55e">
<p>
Current DL methods overfit the training <b>distribution</b>. That is, if they encounter OOD data, they will perform poorly.<br />
</p>
</div>
<div id="outline-container-orgd4f930e" class="outline-3">
<h3 id="orgd4f930e">Conscious processing helps humans deal with OOD settings</h3>
<div class="outline-text-3" id="text-orgd4f930e">
<p>
We are <i>agents</i>, and agents face a dynamic environment &#x2013; particularly because there are other agents! We want our knowledge to generalize across different places, times, input modalities, goals, etc.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-orgfe86c87" class="outline-2">
<h2 id="orgfe86c87">System 1 vs. System 2</h2>
<div class="outline-text-2" id="text-orgfe86c87">
<p>
<b>System 1</b>: Intuitive, fast, unconscious, parallel, non-linguistic, habitual<br />
<b>System 2</b>: Slow, logical, sequential, conscious, linguistic, algorithmic, planning, reasoning<br />
</p>

<p>
Current deep learning systems excel at <a href="thinking_fast_and_slow.html#ID-1a22fb9c-9bc4-4943-9e33-9f08f62409f3">System 1</a> &#x2013; they are fast, intuitive, but brittle. How can we incorporate more <a href="thinking_fast_and_slow.html#ID-62eeec64-5a77-45d2-b386-54fed57e72e0">System 2</a> to allow DL to <i>reason</i> about the world?<br />
</p>
</div>
</div>
<div id="outline-container-org1c52275" class="outline-2">
<h2 id="org1c52275">Implicit vs. verbalizable knowledge</h2>
<div class="outline-text-2" id="text-org1c52275">
<p>
Most of our knowledge is implicit, and not verbalizable. Same for neural networks.<br />
</p>

<p>
Verbalizable knowledge can be explicitly reasoned with, planned with.<br />
</p>
</div>
</div>
<div id="outline-container-org5232300" class="outline-2">
<h2 id="org5232300">Independent mechanisms</h2>
<div class="outline-text-2" id="text-org5232300">
<p>
Hypothesis: We can explain the world by the composition of informationally independent pieces/modules/mechanisms. (Note: not statistically independent, but independent s.t. any causal <a href="causal_inference.html#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a> would affect just one such mechanism.)<br />
</p>
</div>
</div>
<div id="outline-container-orgf4154b1" class="outline-2">
<h2 id="orgf4154b1">Some System 2 inductive priors</h2>
<div class="outline-text-2" id="text-orgf4154b1">
<p>
Sparse causal graph of high-level, semantically meaningful variables.<br />
</p>

<div id="org635f0e7" class="figure">
<p><img src="origin_image.png" alt="Sparse factor graph." /><br />
</p>
</div>

<p>
Semantic variables are <i>causal</i>: agents, intentions, controllable objects, for example.<br />
</p>

<p>
Changes in distribution are due to causal interventions (in the aforementioned high-level semantic space.) Provided we have the right abstractions, it would only take a few words to describe this change.<br />
</p>

<p>
Everything that's happening can be reported in simple language. (Interesting that this is an example of report/access consciousness.) Mapping from semantic variables &lt;=&gt; sentences<br />
</p>

<p>
"generic rules" of how things work are shared across instances &#x2013; need variables / functions / some form of indirection.<br />
</p>

<p>
Stability/robustness in <a href="meaning.html#ID-3841138e-363a-4bc2-b1c4-f5abbf973a54">meaning</a> (e.g. of laws of physics,) even with changes in distribution, vs. things that <i>do</i> change. E.g.: early visual layers are stable after childhood. Later things like object recognition can be adapted to very quickly.<br />
</p>

<p>
Causal chains to explain things are short. (Interesting: connection to <a href="cognitive_dissonance.html#ID-8fb8913e-bdd8-4ece-8386-2978b765d7bf">dissonance reduction</a>: we like simple explanations of the world around us (possibly because it helps us streamline our cognition.))<br />
</p>
</div>
</div>
<div id="outline-container-org2792d76" class="outline-2">
<h2 id="org2792d76">What should the causal variables be?</h2>
<div class="outline-text-2" id="text-org2792d76">
<p>
Position and momentum of every particle: computationally intractable.<br />
</p>

<p>
Take inspiration from scientists (and humans in general): we invent <i>high-level abstractions</i> that make the causal structure of the world simpler.<br />
</p>
</div>
</div>
<div id="outline-container-ID-5b73a108-e867-4b92-9949-832840d52869" class="outline-2">
<h2 id="ID-5b73a108-e867-4b92-9949-832840d52869">Agency to Guide Representation Learning &amp; Disentangling</h2>
<div class="outline-text-2" id="text-orgf2a153c">
<p>
(E. Bengio et al, 2017; V Thomas e al 2017; Kim et al ICML 2019)<br />
</p>

<p>
Independent mechanisms: there are ways to modify a single object in the graph (e.g., you can move a chair ➡️🪑. )<br />
</p>

<p>
Way that we represent actions &lt;=&gt; objects: there's a bijection there.<br />
</p>

<p>
Connected to the psychological notion of <a href="affordances.html#ID-0cebd56a-9669-4ff0-b93e-8e35d05a2d81">affordances</a>: the way we understand objects is by the things we can do with them.<br />
</p>
</div>
</div>
<div id="outline-container-ID-b4821df4-68e3-43b1-a4f1-c212f0b8d922" class="outline-2">
<h2 id="ID-b4821df4-68e3-43b1-a4f1-c212f0b8d922">What causes changes in distribution?</h2>
<div class="outline-text-2" id="text-org09eaefe">
<p>
hypothesis to replace i.i.d. assumption: changes in distribution = consequence of an <b>intervention</b> on one/few <b>causes</b> /mechanisms. So, not identically distributed, but pretty similar, if you're in the right high-level <a href="representations.html#ID-c7ba956c-67ad-4b8e-9c7f-f18bc1b2b4ff">representation</a> space. (E.g. if you put shaded glasses on, all the pixels change in basic RGB space &#x2013; but in some high-level semantic space, only one bit changed!)<br />
</p>
</div>
</div>
<div id="outline-container-org75608d6" class="outline-2">
<h2 id="org75608d6">Causal induction from interventional data</h2>
<div class="outline-text-2" id="text-org75608d6">
<p>
How to handle unknown <a href="causal_inference.html#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a>? <i>infer</i> it.<br />
</p>
</div>
</div>
<div id="outline-container-org32ba41f" class="outline-2">
<h2 id="org32ba41f">Thoughts, Consciousness, Language</h2>
<div class="outline-text-2" id="text-org32ba41f">
<p>
If we want better NLP/NLU, we need to ground language in higher-level concepts.<br />
</p>

<p>
<i>Grounded language learning</i>: BabyAI (2019)<br />
</p>
</div>
</div>
<div id="outline-container-org8dc1829" class="outline-2">
<h2 id="org8dc1829">Core ingredient for conscious processing: <a href="attention.html#ID-2e1955ad-af09-4bcd-8b8d-4a0838e96365">attention</a></h2>
<div class="outline-text-2" id="text-org8dc1829">
<p>
Attention enables us to make <i>dynamic</i> connections to the various different "modules" in the brain. Creates competition between the modules for which deserves attention.<br />
</p>
</div>
</div>
<div id="outline-container-ID-899d0e14-02e5-4858-8f71-8e61e9f59ffa" class="outline-2">
<h2 id="ID-899d0e14-02e5-4858-8f71-8e61e9f59ffa">Going from attention to <a href="consciousness.html#ID-4fba6fb0-e9cc-48b1-875c-a70e1a2dbc9b">consciousness</a></h2>
<div class="outline-text-2" id="text-org8987b1b">
<p>
Dehaene et al. &#x2013; workspace theory of consciousness <a href="#citeproc_bib_item_2">[2]</a><br />
</p>
</div>
</div>

<div id="outline-container-orgd2e9331" class="outline-2">
<h2 id="orgd2e9331">Bibliography</h2>
<div class="outline-text-2" id="text-orgd2e9331">
<style>.csl-left-margin{float: left; padding-right: 0em;}
 .csl-right-inline{margin: 0 0 0 1em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>
    <div class="csl-left-margin">[1]</div><div class="csl-right-inline">B. Schölkopf <i>et al.</i>, “Towards Causal Representation Learning,” Feb. 2021, doi: <a href="https://doi.org/10.48550/arXiv.2102.11107">10.48550/arXiv.2102.11107</a>.</div>
  </div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>
    <div class="csl-left-margin">[2]</div><div class="csl-right-inline">S. Dehaene, J.-P. Changeux, L. Naccache, J. Sackur, and C. Sergent, “Conscious, preconscious, and subliminal processing: A testable taxonomy,” <i>Trends in cognitive sciences</i>, vol. 10, no. 5, pp. 204–211, May 2006, doi: <a href="https://doi.org/10.1016/j.tics.2006.03.007">10.1016/j.tics.2006.03.007</a>.</div>
  </div>
</div>
</div>
</div>


<div id="outline-container-org825c3b6" class="outline-2 references">
<h2 id="org825c3b6">Links to "Towards Causal Representation Learning"</h2>
<div class="outline-text-2" id="text-org825c3b6">
</div>
<div id="outline-container-orgcd499ce" class="outline-3">
<h3 id="orgcd499ce"><a href="paper_notes.html#ID-d4693400-d612-4531-96cb-da0b8d37b4b0">📄 paper notes</a></h3>
<div class="outline-text-3" id="text-orgcd499ce">
<p>
<a href="towards_causal_representation_learning.html#ID-12dfdb1e-d4ed-476b-be04-98cae7a3deaf">Towards Causal Representation Learning</a><br />
<a href="counterfactual_generative_networks.html#ID-22706d1f-6b5c-4c77-acc2-d8c222b395d5">Counterfactual Generative Networks</a><br />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer>
  <p>Made with <span class="heart">♥</span> using
    <a href="https://orgmode.org/">org-mode</a>.
    Source code is available
    <a href="https://github.com/ketan0/digital-laboratory">here</a>.
  </p>
</footer>
<script src="popper.min.js"></script>
<script src="tippy-bundle.umd.min.js"></script>
<script src="tooltips.js"></script>
<script src="setup-theme-switcher.js"></script>
<script src="insert-intext-citation.js"></script>
</div>
</body>
</html>
