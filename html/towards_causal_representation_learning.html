<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-12-16 Sat 01:37 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Towards Causal Representation Learning</title>
<meta name="author" content="Ketan Agrawal" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="syntax.css" />
<link rel="stylesheet" type="text/css" href="styles.css" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
<link rel="manifest" href="/site.webmanifest" />
</head>
<body>
<div id="preamble" class="status">
<header>
    <script src="setup-initial-theme.js"></script>
    <div style="display: flex; flex-direction: row; justify-content: space-between; align-items: center;">
        <a style="color: inherit; text-decoration: none" href="/">
            ketan agrawal
        </a>
        <div>
            <input type="checkbox" id="theme-switcher">
            <label id="theme-switcher-label" for="theme-switcher"></label>
        </div>
    </div>
</header>
</div>
<div id="content" class="content">
<h1 class="title">Towards Causal Representation Learning
<br />
<span class="subtitle">Last modified on May 16, 2022</span>
</h1>
<p>
Yoshua Bengio <a href="https://www.youtube.com/watch?v=rKZJ0TJWvTk">talk</a>. Also, the associated <a href="https://arxiv.org/abs/2102.11107">paper</a>.<br />
</p>

<p>
causal representation learning: the discovery of high-level causal variables from low-level observations.<br />
</p>

<p>
In practice, i.i.d. is a bad assumption. Things don&rsquo;t stay the same distribution as train. Current DL systems are brittle.<br />
</p>

<p>
But&#x2026;what assumption can we replace it with, then?<br />
</p>

<p>
how does the brain break knowledge apart into &ldquo;pieces&rdquo; that can be reused? =&gt; <a href="compositionality.html#ID-b6fafba6-8e57-400d-962c-bf7cc892a41f">compositionality</a> (thinking decomposition into helper functions in programming.) Examples of compositionality include<br />
</p>

<div id="outline-container-orge94e691" class="outline-2">
<h2 id="orge94e691">Systematic Generalization</h2>
<div class="outline-text-2" id="text-orge94e691">
<p>
Current DL methods overfit the training <b>distribution</b>. That is, if they encounter OOD data, they will perform poorly.<br />
</p>
</div>
<div id="outline-container-org6f0c967" class="outline-3">
<h3 id="org6f0c967">Conscious processing helps humans deal with OOD settings</h3>
<div class="outline-text-3" id="text-org6f0c967">
<p>
We are <i>agents</i>, and agents face a dynamic environment &#x2013; particularly because there are other agents! We want our knowledge to generalize across different places, times, input modalities, goals, etc.<br />
</p>
</div>
</div>
</div>
<div id="outline-container-org5f6153c" class="outline-2">
<h2 id="org5f6153c">System 1 vs. System 2</h2>
<div class="outline-text-2" id="text-org5f6153c">
<p>
<b>System 1</b>: Intuitive, fast, unconscious, parallel, non-linguistic, habitual<br />
<b>System 2</b>: Slow, logical, sequential, conscious, linguistic, algorithmic, planning, reasoning<br />
</p>

<p>
Current deep learning systems excel at <a href="thinking_fast_and_slow.html#ID-1a22fb9c-9bc4-4943-9e33-9f08f62409f3">System 1</a> &#x2013; they are fast, intuitive, but brittle. How can we incorporate more <a href="thinking_fast_and_slow.html#ID-62eeec64-5a77-45d2-b386-54fed57e72e0">System 2</a> to allow DL to <i>reason</i> about the world?<br />
</p>
</div>
</div>
<div id="outline-container-orgfb6f6f1" class="outline-2">
<h2 id="orgfb6f6f1">Implicit vs. verbalizable knowledge</h2>
<div class="outline-text-2" id="text-orgfb6f6f1">
<p>
Most of our knowledge is implicit, and not verbalizable. Same for neural networks.<br />
</p>

<p>
Verbalizable knowledge can be explicitly reasoned with, planned with.<br />
</p>
</div>
</div>
<div id="outline-container-org1ea31b2" class="outline-2">
<h2 id="org1ea31b2">Independent mechanisms</h2>
<div class="outline-text-2" id="text-org1ea31b2">
<p>
Hypothesis: We can explain the world by the composition of informationally independent pieces/modules/mechanisms. (Note: not statistically independent, but independent s.t. any causal <a href="causal_inference.html#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a> would affect just one such mechanism.)<br />
</p>
</div>
</div>
<div id="outline-container-org88f4011" class="outline-2">
<h2 id="org88f4011">Some System 2 inductive priors</h2>
<div class="outline-text-2" id="text-org88f4011">
<p>
Sparse causal graph of high-level, semantically meaningful variables.<br />
</p>

<div id="orgda4af09" class="figure">
<p><img src="origin_image.png" alt="Sparse factor graph." /><br />
</p>
</div>

<p>
Semantic variables are <i>causal</i>: agents, intentions, controllable objects, for example.<br />
</p>

<p>
Changes in distribution are due to causal interventions (in the aforementioned high-level semantic space.) Provided we have the right abstractions, it would only take a few words to describe this change.<br />
</p>

<p>
Everything that&rsquo;s happening can be reported in simple language. (Interesting that this is an example of report/access consciousness.) Mapping from semantic variables &lt;=&gt; sentences<br />
</p>

<p>
&ldquo;generic rules&rdquo; of how things work are shared across instances &#x2013; need variables / functions / some form of indirection.<br />
</p>

<p>
Stability/robustness in <a href="meaning.html#ID-3841138e-363a-4bc2-b1c4-f5abbf973a54">meaning</a> (e.g. of laws of physics,) even with changes in distribution, vs. things that <i>do</i> change. E.g.: early visual layers are stable after childhood. Later things like object recognition can be adapted to very quickly.<br />
</p>

<p>
Causal chains to explain things are short. (Interesting: connection to <a href="cognitive_dissonance.html#ID-8fb8913e-bdd8-4ece-8386-2978b765d7bf">dissonance reduction</a>: we like simple explanations of the world around us (possibly because it helps us streamline our cognition.))<br />
</p>
</div>
</div>
<div id="outline-container-org270680f" class="outline-2">
<h2 id="org270680f">What should the causal variables be?</h2>
<div class="outline-text-2" id="text-org270680f">
<p>
Position and momentum of every particle: computationally intractable.<br />
</p>

<p>
Take inspiration from scientists (and humans in general): we invent <i>high-level abstractions</i> that make the causal structure of the world simpler.<br />
</p>
</div>
</div>
<div id="outline-container-ID-5b73a108-e867-4b92-9949-832840d52869" class="outline-2">
<h2 id="ID-5b73a108-e867-4b92-9949-832840d52869">Agency to Guide Representation Learning &amp; Disentangling</h2>
<div class="outline-text-2" id="text-orgec70b8e">
<p>
(E. Bengio et al, 2017; V Thomas e al 2017; Kim et al ICML 2019)<br />
</p>

<p>
Independent mechanisms: there are ways to modify a single object in the graph (e.g., you can move a chair ‚û°Ô∏èü™ë. )<br />
</p>

<p>
Way that we represent actions &lt;=&gt; objects: there&rsquo;s a bijection there.<br />
</p>

<p>
Connected to the psychological notion of <a href="affordances.html#ID-0cebd56a-9669-4ff0-b93e-8e35d05a2d81">affordances</a>: the way we understand objects is by the things we can do with them.<br />
</p>
</div>
</div>
<div id="outline-container-ID-b4821df4-68e3-43b1-a4f1-c212f0b8d922" class="outline-2">
<h2 id="ID-b4821df4-68e3-43b1-a4f1-c212f0b8d922">What causes changes in distribution?</h2>
<div class="outline-text-2" id="text-orgfa9daba">
<p>
hypothesis to replace i.i.d. assumption: changes in distribution = consequence of an <b>intervention</b> on one/few <b>causes</b> /mechanisms. So, not identically distributed, but pretty similar, if you&rsquo;re in the right high-level <a href="representations.html#ID-c7ba956c-67ad-4b8e-9c7f-f18bc1b2b4ff">representation</a> space. (E.g. if you put shaded glasses on, all the pixels change in basic RGB space &#x2013; but in some high-level semantic space, only one bit changed!)<br />
</p>
</div>
</div>
<div id="outline-container-orgd6ddfd6" class="outline-2">
<h2 id="orgd6ddfd6">Causal induction from interventional data</h2>
<div class="outline-text-2" id="text-orgd6ddfd6">
<p>
How to handle unknown <a href="causal_inference.html#ID-d68c5093-d6d6-43b8-a48d-629ade9293b6">intervention</a>? <i>infer</i> it.<br />
</p>
</div>
</div>
<div id="outline-container-org08d2b5d" class="outline-2">
<h2 id="org08d2b5d">Thoughts, Consciousness, Language</h2>
<div class="outline-text-2" id="text-org08d2b5d">
<p>
If we want better NLP/NLU, we need to ground language in higher-level concepts.<br />
</p>

<p>
<i>Grounded language learning</i>: BabyAI (2019)<br />
</p>
</div>
</div>
<div id="outline-container-org270fe93" class="outline-2">
<h2 id="org270fe93">Core ingredient for conscious processing: <a href="attention.html#ID-2e1955ad-af09-4bcd-8b8d-4a0838e96365">attention</a></h2>
<div class="outline-text-2" id="text-org270fe93">
<p>
Attention enables us to make <i>dynamic</i> connections to the various different &ldquo;modules&rdquo; in the brain. Creates competition between the modules for which deserves attention.<br />
</p>
</div>
</div>
<div id="outline-container-ID-899d0e14-02e5-4858-8f71-8e61e9f59ffa" class="outline-2">
<h2 id="ID-899d0e14-02e5-4858-8f71-8e61e9f59ffa">Going from attention to <a href="consciousness.html#ID-4fba6fb0-e9cc-48b1-875c-a70e1a2dbc9b">consciousness</a></h2>
<div class="outline-text-2" id="text-org53490d4">
<p>
Dehaene et al. &#x2013; workspace theory of consciousness <a href="#citeproc_bib_item_1">[1]</a><br />
</p>
</div>
</div>
<div id="outline-container-org0dc8ab7" class="outline-2 no-display">
<h2 id="org0dc8ab7">Link</h2>
<div class="outline-text-2" id="text-org0dc8ab7">
<p>
<sup><a href="#citeproc_bib_item_2">[2]</a></sup><br />
</p>
</div>
</div>
<div id="outline-container-org01f5a76" class="outline-2">
<h2 id="org01f5a76">Bibliography</h2>
<div class="outline-text-2" id="text-org01f5a76">
<style>.csl-left-margin{float: left; padding-right: 0em;}
 .csl-right-inline{margin: 0 0 0 1em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a>
    <div class="csl-left-margin">[1]</div><div class="csl-right-inline">S. Dehaene, J.-P. Changeux, L. Naccache, J. Sackur, and C. Sergent, ‚ÄúConscious, preconscious, and subliminal processing: A testable taxonomy,‚Äù <i>Trends in cognitive sciences</i>, vol. 10, no. 5, pp. 204‚Äì211, May 2006, doi: <a href="https://doi.org/10.1016/j.tics.2006.03.007">10.1016/j.tics.2006.03.007</a>.</div>
  </div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a>
    <div class="csl-left-margin">[2]</div><div class="csl-right-inline">B. Sch√∂lkopf <i>et al.</i>, ‚ÄúTowards Causal Representation Learning,‚Äù Feb. 2021, doi: <a href="https://doi.org/10.48550/arXiv.2102.11107">10.48550/arXiv.2102.11107</a>.</div>
  </div>
</div>
</div>
</div>


<div id="outline-container-orgbdc2f3f" class="outline-2 references">
<h2 id="orgbdc2f3f">Links to &ldquo;Towards Causal Representation Learning&rdquo;</h2>
<div class="outline-text-2" id="text-orgbdc2f3f">
</div>
<div id="outline-container-org53dd649" class="outline-3">
<h3 id="org53dd649"><a href="paper_notes.html#ID-d4693400-d612-4531-96cb-da0b8d37b4b0">üìÑ paper notes</a></h3>
<div class="outline-text-3" id="text-org53dd649">
<p>
<a href="towards_causal_representation_learning.html#ID-12dfdb1e-d4ed-476b-be04-98cae7a3deaf">Towards Causal Representation Learning</a><br />
<a href="counterfactual_generative_networks.html#ID-22706d1f-6b5c-4c77-acc2-d8c222b395d5">Counterfactual Generative Networks</a><br />
<a href="language_learning_as_language_use_a_cross_linguistic_model_of_child_language_development_psycnet.html#ID-8ba4e0bd-82cd-4218-b7cd-ed941e74613e">Language learning as language use: A cross-linguistic model of child language development.</a><br />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer style="font-size: 0.75rem;">
    <p>Made with <span class="heart">‚ô•</span> using
        <a href="https://orgmode.org/">org-mode</a>.
        Source code is available
        <a href="https://github.com/ketan0/digital-laboratory">here</a>.
    </p>
</footer>
<script src="popper.min.js"></script>
<script src="tippy-bundle.umd.min.js"></script>
<script src="tooltips.js"></script>
<script src="setup-theme-switcher.js"></script>
<script src="insert-intext-citation.js"></script>
</div>
</body>
</html>