#+title: counterfactual inference in ML

* Pearl's Causal Hierarchy
So I read the [[pdf:~/Dropbox/Apps/GoodNotes 5/GoodNotes/HCI/On Pearl’s Hierarchy and the Foundations of Causal Inference.pdf][On Pearl’s Hierarchy and the Foundations of Causal Inference]] more deeply, and I think I get the mathematics of it now.
So L1, L2, and L3 represent observational, interventional, and counterfactual layers, respectively.
L2 inference requires more model specification than L1, and L3 requires more than L2.

$\mathcal{L}_1$ supports terms like $P(\mathbf{Y}=\mathbf{y})$
$\mathcal{L}_2$ supports terms like $P(\mathbf{Y_x}=\mathbf{y})$
$\mathcal{L}_3$ supports terms like $P(\mathbf{Y_x}=\mathbf{y},...,\mathbf{Z_w}=\mathbf{z})$

In other words,

$\mathcal{L}_1$ supports no intervention (or the empty intervention, I guess.)
$\mathcal{L}_2$ supports only one intervention.
$\mathcal{L}_3$ supports multiple (potentially) inconsistent interventions.
* The difference between intervention and counterfactuals
Basically, in terms of the math, counterfactual is just an intervention where we have conditioned on past events. It's useful to visualize this in terms of sampling: in the drug example, we first restrict the "pool" to those where $X=0,Y=0$. Then we ask "out of these, how many, if we intervene and set $X = 1$, change to $Y_{X=1} = 1$?"
* Random idea I had for counterfactuals
Google Maps routes -- what happens if you min-edit-distance a certain part of the route?
